{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 데이터 불러오기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==========Scenario B-ARFF==========\n",
      "30s          : (14651, 24)\n",
      "15s-AllinOne : (18758, 24)\n",
      "60s-AllinOne : (15515, 24)\n",
      "15s          : (18758, 24)\n",
      "30s-AllinOne : (14651, 24)\n",
      "120s         : (10782, 24)\n",
      "60s          : (15515, 24)\n",
      "120s-AllinOne : (10782, 24)\n",
      "==========Scenario A2-ARFF==========\n",
      "60s-NO-VPN   : (8580 , 24)\n",
      "15s-VPN      : (9793 , 24)\n",
      "120s-NO-VPN  : (5151 , 24)\n",
      "30s-VPN      : (7734 , 24)\n",
      "60s-VPN      : (6935 , 24)\n",
      "15s-NO-VPN   : (8965 , 24)\n",
      "120s-VPN     : (5631 , 24)\n",
      "30s-NO-VPN   : (6917 , 24)\n",
      "==========Scenario A1-ARFF==========\n",
      "15s-VPN      : (18758, 24)\n",
      "30s-VPN      : (14651, 24)\n",
      "60s-VPN      : (15515, 24)\n",
      "120s-VPN     : (10782, 24)\n",
      "====================================\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from scipy.io import arff\n",
    "\n",
    "folder_path = './ISCX-VPN-NonVPN-2016 CSVs/'\n",
    "all_df = {}\n",
    "all_source = {}\n",
    "\n",
    "for sub_folder in os.listdir(folder_path):\n",
    "  full_path = os.path.join(folder_path, sub_folder)\n",
    "  if os.path.isdir(full_path):\n",
    "    all_folder_df = []\n",
    "    all_folder_source = []\n",
    "    for filename in os.listdir(full_path):\n",
    "      if filename.endswith('.arff'):\n",
    "        file_path = os.path.join(full_path, filename)\n",
    "        extracted = filename.replace('TimeBasedFeatures-Dataset-', '').replace('.arff', '')\n",
    "        try:\n",
    "          data, _ = arff.loadarff(file_path)\n",
    "          df = pd.DataFrame(data)\n",
    "          df['class1'] = df['class1'].str.decode('utf-8')\n",
    "          all_folder_df.append(df)\n",
    "          all_folder_source.append(extracted)\n",
    "        except Exception as e:\n",
    "          print(f\"⚠️ 에러 발생: {sub_folder}/{extracted} → {e}\")\n",
    "          continue\n",
    "    all_df[sub_folder] = all_folder_df\n",
    "    all_source[sub_folder] = all_folder_source\n",
    "  \n",
    "for sub_folder in all_df:\n",
    "  print(f\"=========={sub_folder}==========\")\n",
    "  for df, source in zip(all_df[sub_folder], all_source[sub_folder]):\n",
    "    shape = f\"({df.shape[0]:<5}, {df.shape[1]})\"\n",
    "    print(f\"{source:<12} : {shape}\")\n",
    "print(\"====================================\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 데이터 매핑 함수 선언\n",
    "## (범주형 데이터 숫자형으로 변환)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mapping\n",
    "def col_mapping(df_work: pd.DataFrame):\n",
    "  non_numeric_cols = df_work.select_dtypes(exclude=['number']).columns\n",
    "  total = {}\n",
    "  for col in non_numeric_cols:\n",
    "    normal = {}\n",
    "    reverse = {}\n",
    "    idx = 0\n",
    "    for uni in df[col].unique():\n",
    "      if (pd.isna(uni)): continue\n",
    "      normal[uni] = idx\n",
    "      reverse[idx] = uni\n",
    "      idx += 1\n",
    "    total[col] = {\"normal\": normal, \"reverse\": reverse}\n",
    "  return total\n",
    "\n",
    "def round(df_work: pd.DataFrame, total: dict):\n",
    "  def round_to_nearest(x, valid_values):\n",
    "    return min(valid_values, key=lambda v: abs(v - x))\n",
    "\n",
    "  for col in total:\n",
    "    valid_values = total[col][\"reverse\"].keys()\n",
    "    df_work[col] = df_work[col].apply(lambda x: round_to_nearest(x, valid_values))\n",
    "\n",
    "def numeric_mapping(df_work: pd.DataFrame, total: dict, reverse: bool):\n",
    "  cla = \"reverse\" if reverse else \"normal\"\n",
    "  if (reverse): round(df_work, total)\n",
    "  for col in total:\n",
    "    df_work[col] = df_work[col].map(total[col][cla])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 중복 데이터 처리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def drop_duplicates(df_work: pd.DataFrame):\n",
    "  start = len(df_work)\n",
    "  \n",
    "  df_work.drop_duplicates(inplace=True)\n",
    "  \n",
    "  finish = len(df_work)\n",
    "  print(f\"중복 데이터 처리 : {start} - {start-finish} -> {finish}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 누락 데이터 처리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def processing_missing(df_work: pd.DataFrame):\n",
    "  from sklearn.impute import KNNImputer\n",
    "\n",
    "  start_missing = df_work.isnull().sum()\n",
    "  start_missing = start_missing[start_missing > 0].to_dict()\n",
    "\n",
    "  total = col_mapping(df_work)\n",
    "  numeric_mapping(df_work, total, False)\n",
    "\n",
    "  numeric_cols = df_work.select_dtypes(include='number').columns\n",
    "  df_numeric = df_work[numeric_cols]\n",
    "\n",
    "  imputer = KNNImputer(n_neighbors=3)\n",
    "  df_imputed_numeric = pd.DataFrame(imputer.fit_transform(df_numeric), columns=numeric_cols, index=df_numeric.index)\n",
    "\n",
    "  df_work[numeric_cols] = df_imputed_numeric\n",
    "  \n",
    "  numeric_mapping(df_work, total, True)\n",
    "  \n",
    "  finish_missing = df_work.isnull().sum()\n",
    "  finish_missing = finish_missing[finish_missing > 0].to_dict()\n",
    "  print(f\"누락 데이터 처리 후: {start_missing} -> {finish_missing}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 이상치 처리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def processing_outlier(df_work: pd.DataFrame):\n",
    "  first = len(df_work)\n",
    "\n",
    "  total = col_mapping(df_work)\n",
    "  numeric_mapping(df_work, total, False)\n",
    "\n",
    "  numeric_cols = df_work.select_dtypes(include='number').columns\n",
    "\n",
    "  outlier_indices = set()\n",
    "  for col in numeric_cols:\n",
    "    Q1 = df_work[col].quantile(0.25)\n",
    "    Q3 = df_work[col].quantile(0.75)\n",
    "    IQR = Q3 - Q1\n",
    "\n",
    "    lower_bound = Q1 - 1.5 * IQR\n",
    "    upper_bound = Q3 + 1.5 * IQR\n",
    "\n",
    "    outliers = df_work[(df_work[col] < lower_bound) | (df_work[col] > upper_bound)].index\n",
    "    outlier_indices.update(outliers)\n",
    "\n",
    "  df_work.drop(index=outlier_indices, inplace=True)\n",
    "  df_work.reset_index(drop=True, inplace=True)\n",
    "\n",
    "  numeric_mapping(df_work, total, True)\n",
    "\n",
    "  print(f\"이상치 처리 후: {first} - {len(outlier_indices)} = {len(df_work)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 데이터 전처리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==========Scenario B-ARFF==========\n",
      "=====================================\n",
      "30s - Preprocess\n",
      "중복 데이터 처리 : 14651 - 1013 -> 13638\n",
      "누락 데이터 처리 후: {} -> {}\n",
      "이상치 처리 후: 13638 - 10800 = 2838\n",
      "=====================================\n",
      "15s-AllinOne - Preprocess\n",
      "중복 데이터 처리 : 18758 - 706 -> 18052\n",
      "누락 데이터 처리 후: {} -> {}\n",
      "이상치 처리 후: 18052 - 13644 = 4408\n",
      "=====================================\n",
      "60s-AllinOne - Preprocess\n",
      "중복 데이터 처리 : 15515 - 817 -> 14698\n",
      "누락 데이터 처리 후: {} -> {}\n",
      "이상치 처리 후: 14698 - 12180 = 2518\n",
      "=====================================\n",
      "15s - Preprocess\n",
      "중복 데이터 처리 : 18758 - 684 -> 18074\n",
      "누락 데이터 처리 후: {} -> {}\n",
      "이상치 처리 후: 18074 - 13668 = 4406\n",
      "=====================================\n",
      "30s-AllinOne - Preprocess\n",
      "중복 데이터 처리 : 14651 - 1027 -> 13624\n",
      "누락 데이터 처리 후: {} -> {}\n",
      "이상치 처리 후: 13624 - 10792 = 2832\n",
      "=====================================\n",
      "120s - Preprocess\n",
      "중복 데이터 처리 : 10782 - 1193 -> 9589\n",
      "누락 데이터 처리 후: {} -> {}\n",
      "이상치 처리 후: 9589 - 7471 = 2118\n",
      "=====================================\n",
      "60s - Preprocess\n",
      "중복 데이터 처리 : 15515 - 786 -> 14729\n",
      "누락 데이터 처리 후: {} -> {}\n",
      "이상치 처리 후: 14729 - 12222 = 2507\n",
      "=====================================\n",
      "120s-AllinOne - Preprocess\n",
      "중복 데이터 처리 : 10782 - 1212 -> 9570\n",
      "누락 데이터 처리 후: {} -> {}\n",
      "이상치 처리 후: 9570 - 7451 = 2119\n",
      "=====================================\n",
      "==========Scenario A2-ARFF==========\n",
      "=====================================\n",
      "60s-NO-VPN - Preprocess\n",
      "중복 데이터 처리 : 8580 - 374 -> 8206\n",
      "누락 데이터 처리 후: {} -> {}\n",
      "이상치 처리 후: 8206 - 6463 = 1743\n",
      "=====================================\n",
      "15s-VPN - Preprocess\n",
      "중복 데이터 처리 : 9793 - 413 -> 9380\n",
      "누락 데이터 처리 후: {} -> {}\n",
      "이상치 처리 후: 9380 - 6983 = 2397\n",
      "=====================================\n",
      "120s-NO-VPN - Preprocess\n",
      "중복 데이터 처리 : 5151 - 210 -> 4941\n",
      "누락 데이터 처리 후: {} -> {}\n",
      "이상치 처리 후: 4941 - 3739 = 1202\n",
      "=====================================\n",
      "30s-VPN - Preprocess\n",
      "중복 데이터 처리 : 7734 - 790 -> 6944\n",
      "누락 데이터 처리 후: {} -> {}\n",
      "이상치 처리 후: 6944 - 5323 = 1621\n",
      "=====================================\n",
      "60s-VPN - Preprocess\n",
      "중복 데이터 처리 : 6935 - 412 -> 6523\n",
      "누락 데이터 처리 후: {} -> {}\n",
      "이상치 처리 후: 6523 - 5359 = 1164\n",
      "=====================================\n",
      "15s-NO-VPN - Preprocess\n",
      "중복 데이터 처리 : 8965 - 271 -> 8694\n",
      "누락 데이터 처리 후: {} -> {}\n",
      "이상치 처리 후: 8694 - 6789 = 1905\n",
      "=====================================\n",
      "120s-VPN - Preprocess\n",
      "중복 데이터 처리 : 5631 - 983 -> 4648\n",
      "누락 데이터 처리 후: {} -> {}\n",
      "이상치 처리 후: 4648 - 3715 = 933\n",
      "=====================================\n",
      "30s-NO-VPN - Preprocess\n",
      "중복 데이터 처리 : 6917 - 223 -> 6694\n",
      "누락 데이터 처리 후: {} -> {}\n",
      "이상치 처리 후: 6694 - 5250 = 1444\n",
      "=====================================\n",
      "==========Scenario A1-ARFF==========\n",
      "=====================================\n",
      "15s-VPN - Preprocess\n",
      "중복 데이터 처리 : 18758 - 720 -> 18038\n",
      "누락 데이터 처리 후: {} -> {}\n",
      "이상치 처리 후: 18038 - 13633 = 4405\n",
      "=====================================\n",
      "30s-VPN - Preprocess\n",
      "중복 데이터 처리 : 14651 - 1041 -> 13610\n",
      "누락 데이터 처리 후: {} -> {}\n",
      "이상치 처리 후: 13610 - 10788 = 2822\n",
      "=====================================\n",
      "60s-VPN - Preprocess\n",
      "중복 데이터 처리 : 15515 - 838 -> 14677\n",
      "누락 데이터 처리 후: {} -> {}\n",
      "이상치 처리 후: 14677 - 12162 = 2515\n",
      "=====================================\n",
      "120s-VPN - Preprocess\n",
      "중복 데이터 처리 : 10782 - 1257 -> 9525\n",
      "누락 데이터 처리 후: {} -> {}\n",
      "이상치 처리 후: 9525 - 7705 = 1820\n",
      "=====================================\n"
     ]
    }
   ],
   "source": [
    "# 데이터 젙처리\n",
    "all_df_preprocess = {}\n",
    "for sub_folder in all_df:\n",
    "  all_folder_df_preprocess = []\n",
    "  print(f\"=========={sub_folder}==========\")\n",
    "  for df, source in zip(all_df[sub_folder], all_source[sub_folder]):\n",
    "    df_preprocess = df.copy()\n",
    "    print(\"=====================================\")\n",
    "    print(f\"{source} - Preprocess\")\n",
    "    drop_duplicates(df_preprocess)\n",
    "    processing_missing(df_preprocess)\n",
    "    processing_outlier(df_preprocess)\n",
    "    all_folder_df_preprocess.append(df_preprocess)\n",
    "  print(\"=====================================\")\n",
    "  all_df_preprocess[sub_folder] = all_folder_df_preprocess"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 데이터 전처리 후 shape 변화"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==========Scenario B-ARFF==========\n",
      "30s             : (14651, 24) -> (2838 , 24)\n",
      "15s-AllinOne    : (18758, 24) -> (4408 , 24)\n",
      "60s-AllinOne    : (15515, 24) -> (2518 , 24)\n",
      "15s             : (18758, 24) -> (4406 , 24)\n",
      "30s-AllinOne    : (14651, 24) -> (2832 , 24)\n",
      "120s            : (10782, 24) -> (2118 , 24)\n",
      "60s             : (15515, 24) -> (2507 , 24)\n",
      "120s-AllinOne   : (10782, 24) -> (2119 , 24)\n",
      "==========Scenario A2-ARFF==========\n",
      "60s-NO-VPN      : (8580 , 24) -> (1743 , 24)\n",
      "15s-VPN         : (9793 , 24) -> (2397 , 24)\n",
      "120s-NO-VPN     : (5151 , 24) -> (1202 , 24)\n",
      "30s-VPN         : (7734 , 24) -> (1621 , 24)\n",
      "60s-VPN         : (6935 , 24) -> (1164 , 24)\n",
      "15s-NO-VPN      : (8965 , 24) -> (1905 , 24)\n",
      "120s-VPN        : (5631 , 24) -> (933  , 24)\n",
      "30s-NO-VPN      : (6917 , 24) -> (1444 , 24)\n",
      "==========Scenario A1-ARFF==========\n",
      "15s-VPN         : (18758, 24) -> (4405 , 24)\n",
      "30s-VPN         : (14651, 24) -> (2822 , 24)\n",
      "60s-VPN         : (15515, 24) -> (2515 , 24)\n",
      "120s-VPN        : (10782, 24) -> (1820 , 24)\n",
      "====================================\n"
     ]
    }
   ],
   "source": [
    "for sub_folder in all_df_preprocess:\n",
    "  print(f\"=========={sub_folder}==========\")\n",
    "  for df, df_preprocess, source in zip(all_df[sub_folder], all_df_preprocess[sub_folder], all_source[sub_folder]):\n",
    "    before = f\"({df.shape[0]:<5}, {df.shape[1]})\"\n",
    "    after = f\"({df_preprocess.shape[0]:<5}, {df_preprocess.shape[1]})\"\n",
    "    print(f\"{source:<15} : {before} -> {after}\")\n",
    "print(\"====================================\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_dir = \"preprocessed_data\"\n",
    "os.makedirs(save_dir, exist_ok=True)\n",
    "\n",
    "for sub_folder in all_df_preprocess:\n",
    "    for idx, (df, df_preprocess, source) in enumerate(zip(all_df[sub_folder], all_df_preprocess[sub_folder], all_source[sub_folder])):\n",
    "        filename = f\"{sub_folder}_{source}.csv\"\n",
    "        filepath = os.path.join(save_dir, sub_folder, filename)\n",
    "        os.makedirs(os.path.join(save_dir, sub_folder), exist_ok=True)\n",
    "        df_preprocess.to_csv(filepath, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (basics)",
   "language": "python",
   "name": "basics"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
